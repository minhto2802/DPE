<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>DPE: Diverse Prototypical Ensembles (ICML 2025)</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Unified Google Fonts request -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@600;700&family=Roboto:wght@400;500&family=Roboto+Mono&display=swap"
          rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@600&family=Roboto&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Merriweather&family=Source+Sans+Pro&display=swap"
          rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display&family=Lato&display=swap" rel="stylesheet">

    <style>
        .table-box {
            background-color: #FFF4FD;
            border-radius: 12px;
            padding: 12px;
            margin: 2rem 0;
            box-shadow: 0 2px 6px rgba(0, 0, 0, 0.03);
        }

        .card-section {
            background-color: #fff7ed; /* soft peach background */
            padding: 24px;
            border-radius: 12px;
            margin: 2rem 0;
            font-family: 'Georgia', serif; /* classic serif font */
            color: rgb(32, 0, 0);
            box-shadow: 0 6px 12px rgba(0, 0, 0, 0.1); /* soft shadow */
            /*border: 1px solid #f1e0d3; !* light border for contrast *!*/
        }

        .card-section.summary-section {
            background-color: #edf7f6; /* light teal or choose another distinct tone */
        }

        .card-section.limitation-section {
            background-color: rgba(229, 225, 232, 0.78); /* light teal or choose another distinct tone */
            /*background-color: #e3d6a7; !* light teal or choose another distinct tone *!*/
        }


        .card-section h2 {
            /*font-family: 'Merriweather', serif;*/
            font-family: 'Roboto Mono', monospace;
            font-size: 1.6rem;
            color: rgba(193, 41, 41, 0.73);
            margin-bottom: 1rem;
        }

        .card-section h3 {
            /*font-family: 'Merriweather', serif;*/
            font-family: 'Roboto Mono', monospace;
            font-size: 1.0rem;
            color: rgb(0, 0, 0);
            margin-bottom: 1rem;
        }

        .card-section ul {
            padding-left: 1.25rem;
        }

        .card-section li {
            margin-bottom: 0.5rem;
        }

        .card-section ul, .card-section li {
            font-family: 'Lato', sans-serif;
        }

        .card-section {
            background-color: rgba(255, 255, 255, 0.4);
            padding: 24px;
            border-radius: 12px;
            margin: 2rem 0;
        }

        .card-section h3 {
            text-align: justify;
            font-family: 'Inter', sans-serif;
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 1rem;
        }

        .card-section p {
            text-align: justify;
            font-family: 'Roboto', sans-serif;
            font-size: 1rem;
            color: #333;
            line-height: 1.6;
        }

        .card-section .math {
            text-align: center;
            font-size: 1.1rem;
            font-style: italic;
            margin: 1rem 0;
            font-family: 'Roboto Mono', monospace;
        }

        pre {
            overflow-x: auto;
            white-space: pre-wrap;
            font-family: 'Roboto Mono', monospace;
            font-size: 0.95rem;
            background: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            margin: 1rem 0;
        }

        body {
            font-family: 'Roboto', sans-serif;
            max-width: 900px;
            margin: auto;
            padding: 40px 20px;
            color: #222;
        }

        h1, h2, h3 {
            font-family: 'Inter', sans-serif;
            text-align: center;
        }

        .authors, .institution, .links, .summary {
            text-align: center;
        }

        .button {
            display: inline-block;
            background: rgb(57, 39, 25);
            color: #ffffff;
            padding: 10px 16px;
            margin: 4px;
            border-radius: 999px;
            text-decoration: none;
            font-size: 14px;
        }

        .button:hover {
            background: rgb(151, 93, 57);
        }

        .logo {
            text-align: center;
            margin: 20px;
        }

        .logo img {
            max-width: 300px;
        }

        .code-block {
            text-align: center;
            margin-top: 12px;
            font-family: 'Roboto Mono', monospace;
            color: #c7254e;
            background: #fbeaea;
            display: inline-block;
            padding: 6px 10px;
            border-radius: 4px;
        }

        .summary {
            background: #eef6fb;
            padding: 16px;
            border-radius: 10px;
            margin-top: 30px;
        }

        .visual {
            text-align: center;
            margin-top: 30px;
        }

        .visual img {
            max-width: 100%;
            border-radius: 8px;
        }

        .note {
            font-size: 0.9rem;
            color: #555;
            text-align: center;
            margin-top: 10px;
        }

        img {
            display: block;
            max-width: 100%;
            margin: 20px auto;
        }

        summary {
            font-family: 'Inter', sans-serif;
            font-size: 1.1rem;
            font-weight: 600;
            cursor: pointer;
            background: #eef4ff;
            padding: 10px 16px;
            border-radius: 6px;
            margin: 1.5rem 0;
        }

        details[open] summary {
            background: #dde8ff;
        }

        table {
            background-color: #ffffff;
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
            font-size: 14px;
            font-family: 'Roboto', sans-serif;
        }

        th {
            background: #ebccff;
            padding: 2px;
            font-weight: 600;
            text-align: center;
            font-family: 'Inter', sans-serif;
        }

        td {
            padding: 6px 8px;
            border: 1px solid #ddd;
            text-align: center;
        }

        tr:nth-child(even) {
            background-color: #fafafa;
        }

        tr.highlight {
            background-color: rgba(248, 101, 101, 0.2);
        }

        .std {
            font-size: 11px;
            font-family: 'Roboto Mono', monospace;
            color: #555;
        }

        .bold {
            font-weight: 600;
        }

        .underline {
            text-decoration: underline;
        }

        pre {
            background-color: #f4f4f4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
        }

        .figure-caption-overview {
            background-color: #f0f4ff; /* light blue background */
            padding: 16px;
            border-left: 4px solid #4b6cb7; /* accent bar */
            border-radius: 8px;
            font-size: 14px;
            font-family: 'Source Sans Pro', sans-serif;
            color: #1a1a1a;
            margin-top: 1rem;
            margin-bottom: 2rem;
            text-align: justify;
        }

        .equation-box {
            font-size: 18px;
            background-color: #f3f8ff;
            padding: 10px;
            border-radius: 10px;
            margin: 2rem 0;
            font-family: 'Georgia', serif;
            color: #1f2937;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.05);
            border: 1px solid #dbeafe;
        }

        .equation-box h3 {
            font-family: 'Merriweather', serif;
            font-size: 18px;
            color: #2c5282;
            margin-bottom: 1rem;
        }

        /* Responsive table wrapper */
        .responsive-table {
            overflow-x: auto;
            max-width: 100%;
        }

        /* Keep table readable */
        table {
            width: 100%;
            min-width: 600px;
            border-collapse: collapse;
        }

        /* Scrollable display equations */
        .math-display {
            overflow-x: auto;
            font-size: 1rem;
            padding: 0.5rem 0;
        }

        /* Optional: shrink math font slightly on smaller screens */
        @media (max-width: 768px) {
            .math-display {
                font-size: 0.9rem;
            }
        }
    </style>
</head>

<script>
    window.MathJax = {
        tex: {
            inlineMath: [['\\(', '\\)']],
            displayMath: [['\\[', '\\]'], ['$$', '$$']]
        }
    };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

<body>

<h1>Diversified Prototypical Ensembles</h1>
<p class="institution">Accepted at the International Conference on Machine Learning (ICML) 2025</p>

<div class="authors">
    Minh Nguyen Nhat To · Paul F R Wilson · Viet Nguyen · Mohamed Harmanani · Michael Cooper<br>
    Fahimeh Fooladgar · Purang Abolmaesumi · Parvin Mousavi · Rahul G. Krishnan
</div>

<div class="links">
    <a href="https://arxiv.org/abs/your-id" class="button" target="_blank">
        <svg xmlns="http://www.w3.org/2000/svg" width="34" height="16" viewBox="0 0 90 24"
             style="vertical-align: middle; margin-right: 6px;">
            <text x="0" y="16" font-family="Arial, sans-serif" font-size="24" fill="white">arXiv</text>
        </svg>
        Paper
    </a>

    <a href="https://github.com/your-repo" class="button" target="_blank">
        <!-- SVG goes here -->
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16" width="16" height="16" fill="white"
             style="vertical-align: middle; margin-right: 6px;">
            <path d="M8 0C3.58 0 0 3.58 0 8a8 8 0 005.47 7.59c.4.07.55-.17.55-.38
      0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01
      1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95
      0-.87.31-1.59.82-2.15-.08-.2-.36-1.01.08-2.11 0 0 .67-.21 2.2.82a7.65 7.65 0 012 0c1.53-1.03
      2.2-.82 2.2-.82.44 1.1.16 1.91.08 2.11.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65
      3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.01
      8.01 0 0016 8c0-4.42-3.58-8-8-8z"/>
        </svg>
        Reproduce
    </a>

    <a href="https://openreview.net/forum?id=your_submission_id" class="button" target="_blank">
        <svg height="16" viewBox="0 0 100 100" style="vertical-align: middle; margin-right: 6px;">
            <rect width="100" height="100" fill="#881C1C"/>
            <text x="10" y="35" font-family="Arial, sans-serif" font-size="24" fill="white">Open</text>
            <text x="10" y="60" font-family="Arial, sans-serif" font-size="24" fill="white">Review</text>
            <text x="10" y="85" font-family="Arial, sans-serif" font-size="20" fill="lightgray">.net</text>
        </svg>
        OpenReview
    </a>
</div>

<!--<div class="logo">-->
<!--    <img src="figures/motivation.png" alt="DPE Logo or Teaser Figure">-->
<!--</div>-->

<div style="text-align: center;">
    <div class="code-block">pip install dpe</div>
</div>

<div class="visual">
    <img src="figures/embeddings_figure.png" alt="Diagram Overview of DPE">

    <div class="figure-caption-overview">
        <p>
            <strong>(1)</strong> Binary classification with implicit (unannotated) subgroups. The goal is to detect and
            correct for subpopulation shifts without prior group knowledge.
            <strong>(2)</strong> A <span class="feature-extractor">frozen feature extractor</span>, <i>f(·)</i>, is used
            to extract features.
            <strong>(3)</strong> We train an ensemble of <i>N</i> prototype classifiers per class, using the IPS loss
            (Eq. 4) to encourage diversity and discover latent subpopulations.
            <strong>(4)</strong> A 2D projection shows centroids and sample proximity for the “Landbird” class in <i>Waterbirds</i>.
            Samples closest to each centroid are shown in blue, farther ones in red, and the closest few in dark blue.
            <strong>(5)</strong> Image patches nearest to each centroid confirm that learned prototypes align with
            meaningful subgroups (e.g., birds on land vs. in water).
        </p>
    </div>
</div>

<div class="card-section summary-section">
    <h2>Overview</h2>
    <p>
        Subpopulation shifts occur when the distribution of data subgroups differs between training and deployment
        environments. These shifts can significantly degrade the generalization performance of machine learning models,
        especially when subgroup annotations are unavailable. Existing solutions often rely on reweighting or
        subgroup-specific adjustments, which require prior knowledge or annotations that are difficult to obtain in
        real-world applications.
    </p>
    <p>
        We introduce <strong>Diversified Prototypical Ensembles (DPE)</strong>, a method that improves robustness by
        training a collection of prototype classifiers. Each classifier is designed to focus on different subsets of the
        data and different features. By encouraging diversity among ensemble members, DPE captures multiple decision
        boundaries, enabling better generalization across a wide range of subpopulations.
    </p>
    <p><strong>Key Contributions:</strong></p>
    <ul>
        <li>We propose an end-to-end framework that promotes diversity within an ensemble of prototype-based
            classifiers, each capturing complementary decision rules.
        </li>
        <li>DPE replaces the standard linear head with multiple prototype heads trained using a combination of
            diversification loss and sample resampling.
        </li>
        <li>We demonstrate state-of-the-art performance on nine datasets, achieving improved worst-group accuracy under
            various types of subpopulation shift such as attribute imbalance, spurious correlations, and previously
            unseen attribute values.
        </li>
    </ul>
</div>

<div class="card-section">
    <h2>Motivation</h2>
    <p>
        In real-world datasets, classifiers often learn to rely on shortcuts or spurious features that correlate with
        the labels in the training distribution but fail to hold in deployment. These failures are especially severe
        when certain subgroups are underrepresented or shift at test time, making it difficult for the model to learn a
        single decision boundary that generalizes well.
    </p>
    <p>
        A natural solution is to move beyond a single global decision rule. When a model is encouraged to consider
        multiple distinct ways to separate the data, it becomes more likely that at least one decision rule will align
        with the correct semantics across diverse subpopulations. This forms the key insight behind our approach.
    </p>
    <p>
        Rather than relying on prior knowledge of subgroup structure, we explore the feature space. By
        training multiple classifiers that are explicitly encouraged to be different from one another, we can uncover a
        richer set of decision boundaries that are more robust to distribution shifts.
    </p>

    <div class="over">
        <img src="figures/motivation.png" alt="Diagram Overview of DPE">

        <div class="figure-caption-overview">(a) A single model trained on mixed subgroups learns a biased decision
            boundary
            favoring the majority. (b–d) As ensemble size increases, each model captures different subgroup patterns,
            improving classification across subpopulations.
        </div>

    </div>
</div>


<div class="card-section" id="method">
    <h2 style="text-align: center;">Diversified Prototypical Ensembles</h2>

    <p>
        <strong>DPE</strong> is a two-stage approach for improving robustness to subpopulation shifts by training an
        ensemble of diverse prototype classifiers. DPE first trains a feature extractor \( f: \mathbb{R}^n \to
        \mathbb{R}^d \) using standard empirical risk
        minimization (ERM) on the full training data. This backbone is frozen and then used to extract features for a
        small, class-balanced subset of the validation set. On top of these features, a diversified ensemble of
        prototype classifiers is trained. Notably, no subgroup annotations are required in this second stage, though
        they can improve balance via stratified sampling.
    </p>

    <div class="card-section equation-box">
        <h3>Prototypical Classifier</h3>
        <p>
            Each classifier contains a set of learnable prototypes \( \{p^{(i)}\}_{i=1}^K \), one per class. Given an
            input \( x \), its class probability is computed based on its proximity to each prototype in the latent
            space:
        </p>
        <div class="math-display" style="text-align: center;">
            $$ P(y \mid x) = \frac{\exp\left(-D(f(x), p_y)\right)}{\sum_{i=1}^K \exp\left(-D(f(x), p_i)\right)} $$
        </div>
        <p>
            The distance function \( D \) is scaled Euclidean distance between normalized feature vectors:
        </p>
        <div class="math-display" style="text-align: center;">
            $$ D(x, y) = |d_s| \cdot \left\lVert \frac{x}{\|x\|} - \frac{y}{\|y\|} \right\rVert_2 $$
        </div>
        <p>
            The classification loss for a data-label pair \( (x, y) \) is:
        </p>
        <div class="math-display" style="text-align: center;">
            $$ \mathcal{L}(x, y) = -\log \left( \frac{\exp\left(-D(f(x), p_y)/\tau\right)}{\sum_{i=1}^K
            \exp\left(-D(f(x), p_i)/\tau\right)} \right) $$
        </div>
    </div>

    <div class="card-section equation-box">
        <h3>Prototypical Ensemble</h3>
        <p>
            Instead of a single prototype per class, DPE maintains \( N \) prototypes per class, resulting in an
            ensemble \( \{p^{(k)}_j \mid k = 1, \dots, K; j = 1, \dots, N\} \). The predicted class label is computed by
            averaging predictions across the ensemble:
        </p>
        <div class="math-display" style="text-align: center;">
            $$ \hat{y} = \arg\max_{k \in \{1, \dots, K\}} \frac{1}{N} \sum_{j=1}^N P_j^{(k)}(y \mid x) $$
        </div>
    </div>

    <div class="card-section equation-box">
        <h3>Ensemble Diversification</h3>
        <p>
            To prevent ensemble members from collapsing to similar decision boundaries, DPE uses two diversification
            strategies:
            an <strong>inter-prototype similarity loss</strong> and <strong>bootstrap aggregation</strong> across
            validation samples.
            The similarity loss penalizes correlated prototypes within the same class:
        </p>
        <div class="math-display" style="text-align: center;">
            $$ \mathcal{L}_{\text{IPS}} = \sum_{k=1}^K \sum_{i=1}^{n} \sum_{j=1}^{n} \mathbb{1}_{\{i \ne j\}} \cdot
            \frac{\left| \langle p_i^{(k)}, p_j^{(k)} \rangle \right|}{n \cdot d} $$
        </>
        <p>
            Here, \( \langle \cdot, \cdot \rangle \) denotes the inner product, and \( d \) is the embedding dimension.
            Each prototype is also trained on a different class-balanced bootstrap sample, which encourages coverage of
            different feature subspaces. Together, these mechanisms ensure that the ensemble captures a wide variety of
            subgroup-relevant signals.
        </p>
    </div>
</div>


<div class="card-section" id="evaluation">
    <h2 style="text-align: center;">Strong Generalization Under Distribution Shifts</h2>
    <p>
        We evaluate the robustness of DPE against subpopulation shifts using <strong>nine real-world datasets</strong>
        across vision, language, and medical domains.
        These datasets are chosen to capture common challenges such as spurious
        correlations, attribute imbalance, class imbalance, and attribute generalization, all of which can degrade model
        performance under distribution shift.
        The evaluation includes <strong>Waterbirds</strong>, <strong>CelebA</strong>, <strong>MetaShift</strong>,
        <strong>ImageNetBG</strong>, <strong>NICO++</strong>, <strong>Living17</strong>, <strong>CheXpert</strong>,
        <strong>CivilComments</strong>, and <strong>MultiNLI</strong>, using the training, validation, and test splits
        provided by Yang et al. (2023). These datasets help assess model robustness when attribute or subgroup
        information is available or missing.
    </p>
    <p>
        Across a wide range of subpopulation shift scenarios, <strong>DPE consistently achieves the highest
        worst-group accuracy</strong>, outperforming both classical and state-of-the-art methods. Its effectiveness
        holds under missing subgroup information, challenging shift types like attribute imbalance and
        generalization, and across different backbone strengths.
    </p>


    <div class="card-section equation-box">
        <h3>Robustness With and Without Subgroup Annotations</h3>
        <p>
            When attributes are unavailable during both training and validation, baseline methods degrade significantly.
            For example, ERM achieves 69.1% worst-group accuracy (WGA) on <strong>Waterbirds</strong> and 63.2% on
            <strong>CivilComments</strong>. Methods like CRT and ReWeightCRT also struggle in this setting.
        </p>
        <p>
            In contrast, <strong>DPE</strong> performs consistently well across datasets without requiring subgroup
            annotations. It surpasses DFR (65.2%) and RWY (67.5%), and achieves 76.8% WGA on <strong>CheXpert</strong>
            and 84.6% on <strong>CelebA</strong>.
        </p>
        <p>
            When subgroup annotations are available in the validation set, all methods improve. DPE achieves 94.1% on
            <strong>Waterbirds</strong> and 91.7% on <strong>MetaShift</strong>, outperforming ReWeightCRT (85.1%), DFR
            (81.5%), and GAP (93.8%). On <strong>CivilComments</strong>, DPE reaches 70.8%, exceeding CRT and DFR.
        </p>
    </div>

    <div class='table-box responsive-table' id="wga-no-attributes">
        <h3 style="text-align: center;">Worst-Group Accuracy (Without Subgroup Annotations)</h3>
        <p>
            The top section presents SubpopBench-style baselines using an ERM backbone.
            The bottom section includes recent state-of-the-art methods and our method using a stronger ERM<sup>*</sup>
            backbone.
        </p>
        <table>
            <thead>
            <tr>
                <th>Algorithm</th>
                <th>Waterbirds</th>
                <th>CelebA</th>
                <th>CivilComments</th>
                <th>MultiNLI</th>
                <th>MetaShift</th>
                <th>CheXpert</th>
                <th>ImageNetBG</th>
                <th>NICO++</th>
                <th>Living17</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td><strong>ERM</strong></td>
                <td>69.1 <span class="std">±4.7</span></td>
                <td>57.6 <span class="std">±0.8</span></td>
                <td>63.2 <span class="std">±1.2</span></td>
                <td><b>66.4</b> <span class="std">±2.3</span></td>
                <td>82.1 <span class="std">±0.8</span></td>
                <td>41.7 <span class="std">±3.4</span></td>
                <td>76.8 <span class="std">±0.9</span></td>
                <td>35.0 <span class="std">±4.1</span></td>
                <td>48.0 <span class="std">±1.5</span></td>
            </tr>
            <tr>
                <td><strong>CRT</strong></td>
                <td>76.3 <span class="std">±0.8</span></td>
                <td>69.6 <span class="std">±0.7</span></td>
                <td><b>67.8</b> <span class="std">±0.3</span></td>
                <td>65.4 <span class="std">±0.2</span></td>
                <td>83.1 <span class="std">±0.0</span></td>
                <td>74.6 <span class="std">±0.4</span></td>
                <td><b>78.2</b> <span class="std">±0.5</span></td>
                <td>33.3 <span class="std">±0.0</span></td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>ReWeightCRT</strong></td>
                <td>76.3 <span class="std">±0.2</span></td>
                <td>70.7 <span class="std">±0.6</span></td>
                <td>64.7 <span class="std">±0.2</span></td>
                <td>65.2 <span class="std">±0.2</span></td>
                <td><b><u>85.1</u></b> <span class="std">±0.4</span></td>
                <td><b>75.1</b> <span class="std">±0.2</span></td>
                <td>77.5 <span class="std">±0.7</span></td>
                <td>33.3 <span class="std">±0.0</span></td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>DFR</strong></td>
                <td><b>89.0</b> <span class="std">±0.2</span></td>
                <td><b>73.7</b> <span class="std">±0.8</span></td>
                <td>64.4 <span class="std">±0.1</span></td>
                <td>63.8 <span class="std">±0.0</span></td>
                <td>81.4 <span class="std">±0.1</span></td>
                <td><b><u>75.8</u></b> <span class="std">±0.3</span></td>
                <td>74.4 <span class="std">±1.8</span></td>
                <td><b>38.0</b> <span class="std">±3.8</span></td>
                <td>–</td>
            </tr>
            <tr class="highlight">
                <td><strong>ERM + DPE</strong></td>
                <td><b><u>91.0</u></b> <span class="std">±0.5</span></td>
                <td><b><u>81.9</u></b> <span class="std">±0.2</span></td>
                <td><b><u>69.9</u></b> <span class="std">±0.9</span></td>
                <td><b><u>69.3</u></b> <span class="std">±0.8</span></td>
                <td><b>84.1</b> <span class="std">±1.5</span></td>
                <td>–</td>
                <td><b><u>87.9</u></b> <span class="std">±0.6</span></td>
                <td><b><u>50.0</u></b> <span class="std">±0.0</span></td>
                <td><b><u>54.0</u></b> <span class="std">±4.0</span></td>
            </tr>
            <tr>
                <td><strong>ERM*</strong></td>
                <td>77.9 <span class="std">±3.0</span></td>
                <td>66.5 <span class="std">±2.6</span></td>
                <td><b><u>69.4</u></b> <span class="std">±1.2</span></td>
                <td>66.5 <span class="std">±0.7</span></td>
                <td>80.0 <span class="std">±0.0</span></td>
                <td>75.6 <span class="std">±0.4</span></td>
                <td>86.4 <span class="std">±0.8</span></td>
                <td>33.3 <span class="std">±0.0</span></td>
                <td>53.3 <span class="std">±0.9</span></td>
            </tr>
            <tr>
                <td><strong>RWY</strong></td>
                <td>86.1 <span class="std">±0.7</span></td>
                <td><b>82.9</b> <span class="std">±2.2</span></td>
                <td>67.5 <span class="std">±0.6</span></td>
                <td>68.0 <span class="std">±1.9</span></td>
                <td>–</td>
                <td>–</td>
                <td>–</td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>AFR</strong></td>
                <td><b>90.4</b> <span class="std">±1.1</span></td>
                <td>82.0 <span class="std">±0.5</span></td>
                <td>68.7 <span class="std">±0.6</span></td>
                <td><b><u>73.4</u></b> <span class="std">±0.6</span></td>
                <td>–</td>
                <td>–</td>
                <td>–</td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr class="highlight">
                <td><strong>ERM* + DPE</strong></td>
                <td><b><u>94.1</u></b> <span class="std">±0.2</span></td>
                <td><b><u>84.6</u></b> <span class="std">±0.8</span></td>
                <td><b>68.9</b> <span class="std">±0.6</span></td>
                <td><b>70.9</b> <span class="std">±0.8</span></td>
                <td><b><u>83.6</u></b> <span class="std">±0.9</span></td>
                <td><b><u>76.8</u></b> <span class="std">±0.1</span></td>
                <td><b><u>88.1</u></b> <span class="std">±0.7</span></td>
                <td><b><u>50.0</u></b> <span class="std">±0.0</span></td>
                <td><b><u>63.0</u></b> <span class="std">±1.7</span></td>
            </tr>
            </tbody>
        </table>
    </div>

    <div class="table-box responsive-table" id="wga-with-attributes">
        <h3 style="text-align: center;">Worst-Group Accuracy (With Subgroup Annotations)</h3>
        <p>
            The top section reproduces SubpopBench baselines using the same ERM backbone. The bottom includes results
            from
            original papers and our method (DPE) applied to a stronger ERM<sup>*</sup> backbone.
            Bold and underlined values indicate the best result per group, bold values indicate the second-best.
            DPE denotes our diversified prototypical ensemble. Group Info denotes the use of
            group labels during training and/or validation:
            <strong>❌/❌</strong>: no group info, <strong❌/✔️</strong>: validation group info only,
            <strong>❌/✔️✔️</strong>: group info used for training and tuning, <strong>✔️/✔️</strong>: full group labels
            required.
        </p>
        <table>
            <thead>
            <tr>
                <th>Algorithm</th>
                <th>Group Info (Train/Val)</th>
                <th>Waterbirds</th>
                <th>CelebA</th>
                <th>CivilComments</th>
                <th>MultiNLI</th>
                <th>MetaShift</th>
                <th>CheXpert</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td><strong>ERM</strong></td>
                <td>❌/❌</td>
                <td>69.1 <span class="std">±4.7</span></td>
                <td>57.6 <span class="std">±0.8</span></td>
                <td>63.2 <span class="std">±1.2</span></td>
                <td><b>66.4</b> <span class="std">±2.3</span></td>
                <td>82.1 <span class="std">±0.8</span></td>
                <td>41.7 <span class="std">±3.4</span></td>
            </tr>
            <tr>
                <td><strong>CRT</strong></td>
                <td>❌/✔️</td>
                <td>76.3 <span class="std">±0.8</span></td>
                <td>70.4 <span class="std">±0.4</span></td>
                <td><b>68.5</b> <span class="std">±0.0</span></td>
                <td>65.4 <span class="std">±0.1</span></td>
                <td>83.1 <span class="std">±0.0</span></td>
                <td><b>74.0</b> <span class="std">±0.2</span></td>
            </tr>
            <tr>
                <td><strong>ReWeightCRT</strong></td>
                <td>❌/✔️</td>
                <td>76.3 <span class="std">±0.2</span></td>
                <td>71.1 <span class="std">±0.5</span></td>
                <td>68.2 <span class="std">±0.4</span></td>
                <td>65.3 <span class="std">±0.1</span></td>
                <td><b><u>85.1</u></b> <span class="std">±0.4</span></td>
                <td>73.9 <span class="std">±0.2</span></td>
            </tr>
            <tr>
                <td><strong>DFR</strong></td>
                <td>❌/✔️✔️</td>
                <td><b>89.0</b> <span class="std">±0.2</span></td>
                <td><b>86.3</b> <span class="std">±0.3</span></td>
                <td>66.5 <span class="std">±0.2</span></td>
                <td>63.8 <span class="std">±0.0</span></td>
                <td>81.5 <span class="std">±0.0</span></td>
                <td><b><u>75.4</u></b> <span class="std">±0.6</span></td>
            </tr>
            <tr class="highlight">
                <td><strong>ERM + DPE</strong></td>
                <td>❌/✔️✔️</td>
                <td><b><u>91.0</u></b> <span class="std">±0.4</span></td>
                <td><b><u>87.7</u></b> <span class="std">±0.6</span></td>
                <td><b><u>71.5</u></b> <span class="std">±0.6</span></td>
                <td><b><u>74.8</u></b> <span class="std">±0.3</span></td>
                <td><b>87.9</b> <span class="std">±0.7</span></td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>ERM*</strong></td>
                <td>❌/❌</td>
                <td>77.9 <span class="std">±3.0</span></td>
                <td>66.5 <span class="std">±2.6</span></td>
                <td>69.4 <span class="std">±1.2</span></td>
                <td>66.5 <span class="std">±0.7</span></td>
                <td>80.0 <span class="std">±0.0</span></td>
                <td>75.6 <span class="std">±0.4</span></td>
            </tr>
            <tr>
                <td><strong>Group DRO</strong></td>
                <td>✔️/✔️</td>
                <td>91.4 <span class="std">±1.1</span></td>
                <td>88.9 <span class="std">±2.3</span></td>
                <td>70.0 <span class="std">±2.0</span></td>
                <td><b>77.7</b> <span class="std">±1.4</span></td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>RWG</strong></td>
                <td>✔️/✔️</td>
                <td>87.6 <span class="std">±1.6</span></td>
                <td>84.3 <span class="std">±1.8</span></td>
                <td><b><u>72.0</u></b> <span class="std">±1.9</span></td>
                <td>69.6 <span class="std">±1.0</span></td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>JTT</strong></td>
                <td>❌/✔️</td>
                <td>86.7</td>
                <td>81.1</td>
                <td>69.3</td>
                <td>72.6</td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>CnC</strong></td>
                <td>❌/✔️</td>
                <td>88.5 <span class="std">±0.3</span></td>
                <td>88.8 <span class="std">±0.9</span></td>
                <td>68.9 <span class="std">±2.1</span></td>
                <td>–</td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>SSA</strong></td>
                <td>❌/✔️✔️</td>
                <td>89.0 <span class="std">±0.6</span></td>
                <td>89.8 <span class="std">±1.3</span></td>
                <td>69.9 <span class="std">±2.0</span></td>
                <td>76.6 <span class="std">±0.7</span></td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>DFR*</strong></td>
                <td>❌/✔️✔️</td>
                <td>92.9 <span class="std">±0.2</span></td>
                <td>88.3 <span class="std">±1.1</span></td>
                <td>70.1 <span class="std">±0.8</span></td>
                <td>74.7 <span class="std">±0.7</span></td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>GAP (Last Layer)</strong></td>
                <td>❌/✔️✔️</td>
                <td>93.2 <span class="std">±0.2</span></td>
                <td><b>90.2</b> <span class="std">±0.3</span></td>
                <td>–</td>
                <td>74.3 <span class="std">±0.2</span></td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr>
                <td><strong>GAP (All Layer)</strong></td>
                <td>❌/✔️✔️</td>
                <td><b>93.8</b> <span class="std">±0.1</span></td>
                <td><b>90.2</b> <span class="std">±0.3</span></td>
                <td>–</td>
                <td><b><u>77.8</u></b> <span class="std">±0.6</span></td>
                <td>–</td>
                <td>–</td>
            </tr>
            <tr class="highlight">
                <td><strong>ERM* + DPE</strong></td>
                <td>❌/✔️✔️</td>
                <td><b><u>94.1</u></b> <span class="std">±0.4</span></td>
                <td><b><u>90.3</u></b> <span class="std">±0.7</span></td>
                <td><b>70.8</b> <span class="std">±0.8</span></td>
                <td>75.3 <span class="std">±0.5</span></td>
                <td><b><u>91.7</u></b> <span class="std">±1.3</span></td>
                <td><b><u>76.0</u></b> <span class="std">±0.3</span></td>
            </tr>
            </tbody>
        </table>
    </div>
    <div class="table-box responsive-table" id="standard-accuracy">
        <h3 style="text-align: center;">Standard Accuracy (with Attribute Annotations)</h3>
        <p style="text-align: center; font-size: 0.9rem; margin-top: 4px; cursor: pointer;"
           onclick="toggleAccuracyTable()">(Click to expand)</p>

        <div id="accuracyTable" style="display: none; overflow-x: auto;">
            <p>
                The table below reports average accuracy for each method across datasets.
            </p>

            <table border="1" cellpadding="8" cellspacing="0"
                   style="border-collapse: collapse; width: 100%; font-size: 14px;">
                <thead style="background-color: #f3f4f6;">
                <tr>
                    <th>Algorithm</th>
                    <th>Group Info</th>
                    <th>Waterbirds</th>
                    <th>CelebA</th>
                    <th>CivilComments</th>
                    <th>MultiNLI</th>
                    <th>MetaShift</th>
                    <th>CheXpert</th>
                </tr>
                </thead>
                <tbody>
                <tr>
                    <td>ERM</td>
                    <td>×/×</td>
                    <td>84.1±1.7</td>
                    <td>95.0±0.1</td>
                    <td>85.4±0.2</td>
                    <td>80.9±0.3</td>
                    <td>91.5±0.2</td>
                    <td>88.6±0.7</td>
                </tr>
                <tr>
                    <td>CRT</td>
                    <td>×/✓</td>
                    <td>89.2±0.1</td>
                    <td>94.1±0.1</td>
                    <td>83.0±0.0</td>
                    <td>80.2±0.0</td>
                    <td>91.5±0.0</td>
                    <td>79.1±0.1</td>
                </tr>
                <tr>
                    <td>ReWeightCRT</td>
                    <td>×/✓</td>
                    <td>89.4±0.3</td>
                    <td>94.2±0.1</td>
                    <td>83.4±0.0</td>
                    <td>80.2±0.0</td>
                    <td>91.3±0.1</td>
                    <td>79.0±0.0</td>
                </tr>
                <tr>
                    <td>DFR</td>
                    <td>×/✓✓</td>
                    <td>92.2±0.2</td>
                    <td>91.2±0.1</td>
                    <td>81.3±0.0</td>
                    <td>80.2±0.0</td>
                    <td>90.5±0.4</td>
                    <td>78.9±0.2</td>
                </tr>
                <tr>
                    <td>ERM + DPE</td>
                    <td>×/✓✓</td>
                    <td>92.5±0.2</td>
                    <td>89.8±0.2</td>
                    <td>82.2±0.2</td>
                    <td>81.3±0.2</td>
                    <td>91.2±0.1</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>ERM*</td>
                    <td>×/×</td>
                    <td>92.1±0.2</td>
                    <td>94.0±0.2</td>
                    <td>83.3±1.4</td>
                    <td>81.9±0.2</td>
                    <td>93.2±0.1</td>
                    <td>79.4±0.3</td>
                </tr>
                <tr>
                    <td>Group DRO</td>
                    <td>✓/✓</td>
                    <td>93.5</td>
                    <td>92.9</td>
                    <td>88.9</td>
                    <td>81.4</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>RWG</td>
                    <td>✓/✓</td>
                    <td>–</td>
                    <td>–</td>
                    <td>–</td>
                    <td>–</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>JTT</td>
                    <td>×/✓</td>
                    <td>93.3</td>
                    <td>88.0</td>
                    <td>91.1</td>
                    <td>78.6</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>CnC</td>
                    <td>×/✓</td>
                    <td>90.9±0.1</td>
                    <td>89.9±0.5</td>
                    <td>81.7±0.5</td>
                    <td>–</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>SSA</td>
                    <td>×/✓✓</td>
                    <td>92.2±0.9</td>
                    <td>92.8±0.1</td>
                    <td>88.2±2.0</td>
                    <td>79.9±0.87</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>DFR*</td>
                    <td>×/✓✓</td>
                    <td>94.2±0.4</td>
                    <td>91.3±0.3</td>
                    <td>87.2±0.3</td>
                    <td>82.1±0.2</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>GAP (Last Layer)</td>
                    <td>×/✓✓</td>
                    <td>94.6±0.2</td>
                    <td>91.7±0.2</td>
                    <td>–</td>
                    <td>81.9±0.0</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr>
                    <td>GAP (All Layer)</td>
                    <td>×/✓✓</td>
                    <td>95.6±0.1</td>
                    <td>91.5±0.1</td>
                    <td>–</td>
                    <td>82.5±0.1</td>
                    <td>–</td>
                    <td>–</td>
                </tr>
                <tr style="background-color: #f9fafb;">
                    <td>ERM* + DPE</td>
                    <td>×/✓✓</td>
                    <td>96.0±0.1</td>
                    <td>91.9±0.3</td>
                    <td>81.6±0.2</td>
                    <td>81.6±0.2</td>
                    <td>93.8±0.5</td>
                    <td>79.0±0.2</td>
                </tr>
                </tbody>
            </table>
        </div>
    </div>

    <script>
        function toggleAccuracyTable() {
            const table = document.getElementById('accuracyTable');
            table.style.display = table.style.display === 'none' ? 'block' : 'none';
        }
    </script>


    <script>
        function toggleAccuracyTable() {
            const table = document.getElementById('accuracyTable');
            table.style.display = table.style.display === 'none' ? 'block' : 'none';
        }
    </script>

    <div class="card-section equation-box">
        <h3>Attribute Imbalance and Generalization</h3>
        <p>
            Attribute imbalance and attribute generalization remain difficult for most methods. As reported in prior
            benchmarks, even strong methods like GroupDRO and JTT fail to show consistent gains in these scenarios.
        </p>
        <p>
            DPE addresses these challenges directly. It achieves the highest WGA on both attribute-imbalanced datasets
            like <strong>CheXpert</strong> and <strong>CivilComments</strong>, and attribute-generalization benchmarks
            such as <strong>NICO++</strong> and <strong>Living17</strong>. By promoting diversity across prototypes, DPE
            generalizes beyond the dominant attributes present in training.
        </p>
    </div>

    <div class="card-section equation-box">
        <h3>Disentangling Backbone Effects</h3>
        <p>
            To isolate the effect of DPE from that of the feature extractor, the authors compare DPE on both standard
            ERM and an enhanced ERM<sup>*</sup> backbone. DPE consistently improves WGA regardless of the backbone.
        </p>
        <p>
            On <strong>Waterbirds</strong>, WGA improves from 69.1% (ERM) and 77.9% (ERM<sup>*</sup>) to 91.0% and 94.1%
            with DPE, respectively. On <strong>CelebA</strong>, WGA increases from 57.6% to 81.9% (ERM+DPE), and from
            66.5% to 84.6% (ERM<sup>*</sup>+DPE). Similarly, on <strong>MultinLI</strong>, WGA improves from 66.4% to
            69.3% and from 66.5% to 70.9% when DPE is added.
        </p>
        <p>
            These results confirm that DPE’s improvements come from prototype diversification, not just better
            representations, making it a modular enhancement applicable across training pipelines.
        </p>
    </div>


</div>

<div class="card-section" id="prototype-alignment">
    <h2 style="text-align: center;">Exploratory Analysis of Prototype–Subgroup Alignment</h2>
    <p>
        To better understand the semantic structure captured by the Diversified Prototypical Ensemble (DPE), we
        conducted an exploratory analysis on the <strong>Waterbirds</strong> dataset. For each prototype, we retrieved
        its top-10 closest validation samples and analyzed the emerging patterns using ChatGPT. As visualized below, the
        learned prototypes show consistent alignment with ecologically or visually meaningful subpopulations, even
        though no subgroup labels were used during training.
    </p>
    <p>
        In the <strong>Waterbirds</strong> class, the prototypes capture structured concepts such as aquatic divers,
        large-bodied birds in flight, and compact seabirds in bamboo-heavy or terrestrial environments. In the <strong>Landbirds</strong>
        class, the clusters reflect postural cues, background settings, and spurious correlations like songbirds
        appearing in human-made or beach scenes.
    </p>
    <p>
        These findings suggest that DPE facilitates implicit subgroup discovery through diversification, which may
        contribute to its strong performance on worst-group accuracy.
    </p>

    <!-- Waterbirds Figure -->
    <div class="visual" style="text-align: center;">
        <img src="figures/supp/waterbird_prototypes_with_chatgpt_descriptions.jpg" alt="Waterbird Prototypes"
             style="width: 100%; max-width: 900px; border-radius: 8px;"/>
        <div class="figure-caption-overview">
            <strong>Waterbird Prototypes.</strong> Each row shows the top-10 validation samples nearest to one
            of the prototypes learned for the Waterbirds class. Patterns include aquatic divers, birds in flight,
            terrestrial seabirds, and upright gulls, as interpreted by ChatGPT.
        </div>
    </div>

    <!-- Landbirds Figure -->
    <div class="visual" style="text-align: center;">
        <img src="figures/supp/landbird_prototypes_with_chatgpt_descriptions.jpg" alt="Landbird Prototypes"
             style="width: 100%; max-width: 900px; border-radius: 8px;"/>
        <div class="figure-caption-overview">
            <strong>Landbird Prototypes.</strong> Each row shows the top-10 validation samples closest to a DPE
            prototype for the Landbirds class. Learned clusters correspond to postural variation, ecological context,
            and background-driven biases such as birds in human-made settings.
        </div>
    </div>
</div>

<div class="card-section" id="diversification-ablation">
    <h2 style="text-align: center;">Effect of Ensemble Diversification Strategies</h2>

    <p>
        The figure below extends the results from the main paper (Figure 5) to four datasets, illustrating how different
        ensemble diversification strategies impact both worst-group accuracy and balanced accuracy as the number of
        ensemble members increases. The comparison includes three methods:
    </p>
    <ol>
        <li>Training each prototype on a fixed data subset (no diversification)</li>
        <li>Training on random subsets for each ensemble member</li>
        <li>Combining random subset selection with inter-prototype similarity loss</li>
    </ol>
    <p>
        The combined strategy consistently yields the best worst-group accuracy across datasets. This highlights the
        importance of combining both explicit (loss-based) and implicit (sampling-based) diversification to ensure broad
        coverage of the data distribution and robustness to subpopulation shifts.
    </p>

    <div class="visual" style="text-align: center;">
        <img src="figures/cov_loss_ablation.jpg" alt="Effect of diversification methods"
             style="width: 100%; max-width: 900px; border-radius: 8px;"/>
        <div class="figure-caption-overview">(a) <strong>Effect of different ensemble diversification methods</strong>
            on performance with increasing
            ensemble size. The combination of sampling and similarity loss provide the most robust improvement in
            worst-group and balanced accuracy.
        </div>
    </div>
</div>

<div class="card-section limitation-section" id="limitations">
    <h2 style="text-align: center;">Limitations</h2>
    <p>
        DPE introduces additional complexity due to its ensemble structure and hyperparameters, though this is offset by
        improved robustness without requiring subgroup labels. The method remains efficient, adding only a few minutes
        per prototype when trained on pre-extracted features. While empirical results are strong, a formal theoretical
        explanation for why prototype diversification improves worst-group accuracy is lacking. Our exploratory analysis
        suggests semantic alignment with latent subgroups, but a rigorous understanding remains an open research
        direction.
    </p>
</div>


<h2>BibTeX</h2>
<pre>
@inproceedings{to2025dpe,
  title={Diverse Prototypical Ensembles Improve Robustness to Subpopulation Shift},
  author={To, Minh Nguyen Nhat and Wilson, Paul F R and Nguyen, Viet and Harmanani, Mohamed and Cooper, Michael and
          Fooladgar, Fahimeh and Abolmaesumi, Purang and Mousavi, Parvin and Krishnan, Rahul},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2025}
}
</pre>


</body>
</html>

